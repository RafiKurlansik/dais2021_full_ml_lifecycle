{"cells":[{"cell_type":"markdown","source":["title: End-to-End MLOps demo with MLFlow, Feature Store and Auto ML, part 0 - Setup\n\nauthors:\n- Rafi Kurlansik\n\nedits:\n- Stephanie Rivera\n\ntags:\n- python\n- telco\n- csv\n- delta\n\ncreated_at: 2021-05-01\n\nupdated_at: 2021-08-03\n\ntldr: End-to-end demo of Databricks for MLOps, including MLflow, the registry, webhooks, scoring, feature store and auto ML. Part 0 - setup of data and Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b188afe-45ff-4fe1-9b1d-1e5024ebf12c"}}},{"cell_type":"markdown","source":["# Notebook Links\n- AWS demo.cloud: [https://demo.cloud.databricks.com/#notebook/10166850](https://demo.cloud.databricks.com/#notebook/10166850)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25c3cc48-3eee-4d2b-af48-cce2927d5958"}}},{"cell_type":"markdown","source":["### Setup\n\nIn this case we'll grab a CSV from the web, but we could also use Python or Spark to read data from databases or cloud storage."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7760ba6-b432-4a37-a9fc-c9d83e86bb20"}}},{"cell_type":"code","source":["%sh\nwget https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a9eac4-f7e6-480b-8fe1-fab968280cdf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Load into Delta Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b98e4a82-0de6-4008-a603-d63564867f43"}}},{"cell_type":"markdown","source":["#### Path configs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4485c36-8b49-4672-937b-b3e9ca4d1e7f"}}},{"cell_type":"code","source":["# Load libraries\nimport shutil\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pyspark.sql.functions import col, when\nfrom pyspark.sql.types import StructType,StructField,DoubleType, StringType, IntegerType, FloatType\n\n# Set config for database name, file paths, and table names\ndatabase_name = 'sr_ibm_telco_churn'\n\n# Move file from driver to DBFS\nuser = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\ndriver_to_dbfs_path = 'dbfs:/home/{}/ibm-telco-churn/Telco-Customer-Churn.csv'.format(user)\ndbutils.fs.cp('file:/databricks/driver/Telco-Customer-Churn.csv', driver_to_dbfs_path)\n\n# Paths for various Delta tables\nbronze_tbl_path = '/home/{}/ibm-telco-churn/bronze/'.format(user)\nsilver_tbl_path = '/home/{}/ibm-telco-churn/silver/'.format(user)\nautoml_tbl_path = '/home/{}/ibm-telco-churn/automl-silver/'.format(user)\ntelco_preds_path = '/home/{}/ibm-telco-churn/preds/'.format(user)\n\nbronze_tbl_name = 'bronze_customers'\nsilver_tbl_name = 'silver_customers'\nautoml_tbl_name = 'gold_customers'\ntelco_preds_tbl_name = 'telco_preds'\n\n# Delete the old database and tables if needed\n_ = spark.sql('DROP DATABASE IF EXISTS {} CASCADE'.format(database_name))\n\n# Create database to house tables\n_ = spark.sql('CREATE DATABASE {}'.format(database_name))\n# Drop any old delta lake files if needed (e.g. re-running this notebook with the same bronze_tbl_path and silver_tbl_path)\nshutil.rmtree('/dbfs'+bronze_tbl_path, ignore_errors=True)\nshutil.rmtree('/dbfs'+silver_tbl_path, ignore_errors=True)\nshutil.rmtree('/dbfs'+telco_preds_path, ignore_errors=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20cb7c24-1d71-4e6f-bee4-d20372e429b5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Read and display"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a72615b4-ee6a-4f71-952c-77d0626756f8"}}},{"cell_type":"code","source":["# Define schema\nschema = StructType([\n  StructField('customerID', StringType()),\n  StructField('gender', StringType()),\n  StructField('seniorCitizen', DoubleType()),\n  StructField('partner', StringType()),\n  StructField('dependents', StringType()),\n  StructField('tenure', DoubleType()),\n  StructField('phoneService', StringType()),\n  StructField('multipleLines', StringType()),\n  StructField('internetService', StringType()), \n  StructField('onlineSecurity', StringType()),\n  StructField('onlineBackup', StringType()),\n  StructField('deviceProtection', StringType()),\n  StructField('techSupport', StringType()),\n  StructField('streamingTV', StringType()),\n  StructField('streamingMovies', StringType()),\n  StructField('contract', StringType()),\n  StructField('paperlessBilling', StringType()),\n  StructField('paymentMethod', StringType()),\n  StructField('monthlyCharges', DoubleType()),\n  StructField('totalCharges', DoubleType()),\n  StructField('churnString', StringType())\n  ])\n\n# Read CSV, write to Delta and take a look\nbronze_df = spark.read.format('csv').schema(schema).option('header','true')\\\n               .load(driver_to_dbfs_path)\n\nbronze_df.write.format('delta').mode('overwrite').save(bronze_tbl_path)\n\ndisplay(bronze_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a348565-8c75-4c12-afbe-b8f77d8f0646"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Create bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb89490f-7798-4a95-bdfb-6ab873289867"}}},{"cell_type":"code","source":["# Create bronze table\n_ = spark.sql('''\n  CREATE TABLE `{}`.{}\n  USING DELTA \n  LOCATION '{}'\n  '''.format(database_name,bronze_tbl_name,bronze_tbl_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2eabbca-5869-4f89-a140-e2c5266f5257"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b97c4b1-4233-4dc4-9a07-a38f58790872"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"00a_lakehouse_etl","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4005514664376735}},"nbformat":4,"nbformat_minor":0}
